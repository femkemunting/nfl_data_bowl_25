{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from pyts.approximation import SymbolicAggregateApproximation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pyts')\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, SimpleRNN, Dense, Concatenate, BatchNormalization, Dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in all files and limiting them to just the before_snap data\n",
    "# change the file name depending on where you have downloaded the data to\n",
    "\n",
    "for i in range(1, 10):\n",
    "    data_chunks = pd.read_csv(f\"data/all_tracking_compressed/tracking_week_{i}.csv\", chunksize = 1000)\n",
    "    df_all = pd.concat(data_chunks)\n",
    "    df = df_all[df_all[\"frameType\"] == \"BEFORE_SNAP\"]\n",
    "    df.to_csv(f\"data/all_tracking_compressed/tracking_week_{i}.csv\")\n",
    "\n",
    "\n",
    "# combining all dataframes into one\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# processing each in chunks\n",
    "for week in range(1, 10):\n",
    "    file_name = f\"data/all_tracking_compressed/tracking_week_{week}.csv\"\n",
    "    chunk_iter = pd.read_csv(file_name, chunksize=100000)\n",
    "    for chunk in chunk_iter:\n",
    "        # adding \"week\" column so we can keep track of which week each row belongs to\n",
    "        chunk['week'] = week\n",
    "        # Append the chunk to the combined DataFrame\n",
    "        combined_df = pd.concat([combined_df, chunk], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "#combined_df.to_csv(\"combined_tracking_data.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(combined_df.head())\n",
    "\n",
    "# limit teams to those in the NFC South\n",
    "\n",
    "nfc_south = [\"CAR\", \"NO\", \"ATL\", \"TB\"]\n",
    "\n",
    "combined_df = combined_df[combined_df[\"club\"].isin(nfc_south)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert play directions so that everything is \"moving\" in the same direction (left to right)\n",
    "\n",
    "combined_df[\"ToLeft\"] = combined_df[\"playDirection\"] == \"left\"\n",
    "\n",
    "combined_df['X_std'] = np.where(combined_df['ToLeft'], 120 - combined_df['x'], combined_df['x']) - 10\n",
    "combined_df['Y_std'] = np.where(combined_df['ToLeft'], 160 / 3 - combined_df['y'], combined_df['y'])\n",
    "\n",
    "# standardize orientation\n",
    "combined_df['o_std'] = np.where(combined_df['ToLeft'], (combined_df['o'] + 180) % 360, combined_df['o'])\n",
    "\n",
    "# standardize direction of motion (dir)\n",
    "combined_df['dir_std'] = np.where(combined_df['ToLeft'], (combined_df['dir'] + 180) % 360, combined_df['dir'])\n",
    "\n",
    "# limit orientations to those within the field\n",
    "\n",
    "combined_df = combined_df[combined_df[\"X_std\"] >= 0]\n",
    "combined_df = combined_df[combined_df[\"X_std\"] <= 120]\n",
    "combined_df = combined_df[combined_df[\"Y_std\"] >= 0]\n",
    "combined_df = combined_df[combined_df[\"Y_std\"] <= 53.33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_features(group):\n",
    "    group['time_diff'] = group['time'].diff().dt.total_seconds().bfill().ffill()\n",
    "    group['delta_x'] = group['X_std'].diff()\n",
    "    group['delta_y'] = group['Y_std'].diff()\n",
    "    group['distance'] = np.sqrt(group['delta_x'] ** 2 + group['delta_y'] ** 2)\n",
    "    group['speed'] = group['distance'] / group['time_diff']\n",
    "    group['speed_diff'] = group['speed'].diff()\n",
    "    group['acceleration'] = group['speed_diff'] / group['time_diff']\n",
    "    group['movement_angle'] = np.arctan2(group['delta_y'], group['delta_x'])\n",
    "    return group\n",
    "\n",
    "if 'X_std' in combined_df.columns and 'Y_std' in combined_df.columns:\n",
    "    grouped_data = combined_df.groupby(['gameId', 'nflId', 'playId'], group_keys=False).apply(compute_group_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform SAX; symbolic approximate aggregation\n",
    "\n",
    "sax_representations = []\n",
    "\n",
    "alphabet_size = 15  # chose 15 after messing around with lower numbers; higher alphabet gives more granularity\n",
    "n_points = 10  # Fixed number of points to resample each sequence\n",
    "\n",
    "# Group by gameId, nflId, and playId\n",
    "for (gameId, nflId, playId), group in combined_df.groupby(['gameId', 'nflId', 'playId']):\n",
    "    features = group[['X_std', 'Y_std', 'a', 's', 'o', 'dir']]\n",
    "    feature_names = features.columns.tolist()\n",
    "    \n",
    "    # Resample each feature to a fixed number of points\n",
    "    resampled_features = []\n",
    "    for column in feature_names:\n",
    "        original = features[column].values\n",
    "        resampled = np.interp(\n",
    "            np.linspace(0, len(original) - 1, n_points),  # New indices\n",
    "            np.arange(len(original)),                    # Original indices\n",
    "            original                                     # Original values\n",
    "        )\n",
    "        resampled_features.append(resampled)\n",
    "    \n",
    "    # Stack resampled features\n",
    "    resampled_features = np.array(resampled_features)\n",
    "    \n",
    "    # Perform SAX transformation\n",
    "    sax = SymbolicAggregateApproximation(n_bins=alphabet_size, strategy='quantile')\n",
    "    sax_words = sax.fit_transform(resampled_features)\n",
    "    \n",
    "    # Create a dictionary for SAX results\n",
    "    sax_representation = {'gameId': gameId, 'nflId': nflId, 'playId': playId}\n",
    "    for i, name in enumerate(feature_names):\n",
    "        sax_representation[f'sax_{name}'] = ''.join(sax_words[i])\n",
    "    \n",
    "    # Append the SAX representation\n",
    "    sax_representations.append(sax_representation)\n",
    "\n",
    "# Convert SAX results to a DataFrame\n",
    "df_sax = pd.DataFrame(sax_representations)\n",
    "\n",
    "data = combined_df.merge(df_sax, on=['gameId', 'nflId', 'playId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is now in groups but there is still a row for each time point; only take one entry from each group\n",
    "\n",
    "grouped_data = combined_df.groupby(['gameId', 'nflId', 'playId', \"sax_X_std\", \"sax_Y_std\", \"sax_a\", \"sax_s\", \"sax_o\", \"sax_dir\"]).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with other datasets\n",
    "\n",
    "plays = pd.read_csv(\"data/plays.csv\")\n",
    "players = pd.read_csv(\"data/players.csv\")\n",
    "player_play = pd.read_csv(\"data/player_play.csv\")\n",
    "games = pd.read_csv(\"data/games.csv\")\n",
    "\n",
    "grouped_data = grouped_data.merge(plays, on = [\"gameId\", \"playId\"])\n",
    "grouped_data = grouped_data.merge(players, on = \"nflId\")\n",
    "grouped_data = grouped_data.merge(player_play, on = [\"gameId\", \"playId\", \"nflId\"])\n",
    "grouped_data = grouped_data.merge(games, on = \"gameId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert height to height in inches\n",
    "\n",
    "def height_to_inches(height):\n",
    "    try:\n",
    "        # Split the height into feet and inches\n",
    "        feet, inches = map(int, height.split('-'))\n",
    "        # Convert feet to inches and add the remaining inches\n",
    "        return feet * 12 + inches\n",
    "    except (ValueError, AttributeError):\n",
    "        # Return None for invalid or missing data\n",
    "        return None\n",
    "\n",
    "# Apply the function to the height column\n",
    "grouped_data['height_inches'] = grouped_data['height'].apply(height_to_inches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit data to only offensive positions\n",
    "\n",
    "offensive_positions = [\"T\", \"G\", \"C\", \"QB\", \"RB\", \"FB\", \"WR\"]\n",
    "grouped_data = grouped_data[grouped_data[\"position\"].isin(offensive_positions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add target variable; playOutcome\n",
    "\n",
    "def determine_play_outcome(row):\n",
    "    if row['routeRan'] == 'Slant':\n",
    "        return 'Slant'\n",
    "    elif pd.notnull(row['routeRan']) and row['hadRushAttempt'] == 0:\n",
    "        return row['routeRan']  # Use the route name for other routes\n",
    "    elif pd.isnull(row['routeRan']) and row['hadRushAttempt'] == 1:\n",
    "        return 'Rush'\n",
    "    elif pd.isnull(row['routeRan']) and row['hadRushAttempt'] == 0:\n",
    "        return 'None'\n",
    "    elif pd.notnull(row['routeRan']) and row['hadRushAttempt'] == 1:\n",
    "        return f\"{row['routeRan']} and Rush\"\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply the function to create the playOutcome column\n",
    "grouped_data['playOutcome'] = grouped_data.apply(determine_play_outcome, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update time metrics\n",
    "\n",
    "grouped_data['time'] = pd.to_datetime(grouped_data['time'], errors='coerce')\n",
    "grouped_data['month'] = grouped_data['time'].dt.month\n",
    "grouped_data['day'] = grouped_data['time'].dt.day\n",
    "#grouped_data['day_of_week'] = grouped_data['time'].dt.dayofweek  # Monday=0, Sunday=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data[\"age\"] = 2024 - pd.to_datetime(grouped_data[\"birthDate\"], format = \"mixed\").dt.year\n",
    "grouped_data[\"gameClock_min\"] = grouped_data[\"gameClock\"].str[:2].astype(int)\n",
    "grouped_data[\"gameClock_sec\"] = grouped_data[\"gameClock\"].str[-2:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to only columnts that are relevant to pre-snap info\n",
    "\n",
    "grouped_data = grouped_data[grouped_data.columns[~grouped_data.columns.str.endswith('_y')]]\n",
    "grouped_data.columns = [col.replace('_x', '') for col in grouped_data.columns]\n",
    "\n",
    "data = grouped_data[[\"playOutcome\", \"gameId\", \"playId\", \"nflId\", \"frameId\", \"time\", \"week\", \"norm_o\", \"o_minus_dir\", \"quarter\", \"down\", \"yardsToGo\", \"possessionTeam\", \"defensiveTeam\", \"yardlineNumber\", \"gameClock_min\", \"gameClock_min\", \"preSnapHomeScore\", \"preSnapVisitorScore\", \"absoluteYardlineNumber\", \"preSnapHomeTeamWinProbability\", \"expectedPoints\", \"offenseFormation\", \"receiverAlignment\", \"routeRan\", \"pff_defensiveCoverageAssignment\", \"height_inches\", \"weight\", \"position\", \"age\", \"sax_X_std\", \"sax_Y_std\", \"sax_a\", \"sax_s\", \"sax_o\", \"sax_dir\", \"time_diff\", \"distance\", \"movement_angle_degrees\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/reduced_sax_presnap.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3c82d000bb8a1750346fb87c49af9771012dc5d265e5e793ee442432926adf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
